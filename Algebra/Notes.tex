\documentclass[letterpaper]{article}
\input{../preamble.tex}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage[hidelinks]{hyperref}

\pagestyle{fancy}
\fancyhf{}
\rhead{Math 103A}
\chead{June 29th, 2021}
\lhead{Course Notes}
\rfoot{\thepage}

\setlength{\parindent}{0pt}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{Math 103 Notes}
            
        \vspace{0.5cm}
        \LARGE
        Modern Algebra
            
        \vspace{1.5cm}
            
        \vfill
            
        Summer Session 1 2021\\
        Taught by Professor Kyle Meyer and Alireza Golsefidy
    \end{center}
\end{titlepage}

\pagenumbering{gobble}

\newpage 

\begingroup
    \renewcommand\contentsname{Table of Contents}
    \tableofcontents
\endgroup

\newpage
\pagenumbering{arabic}

\section{Review: Equivalence Relations}
We will go over topics covered in other courses that will be used in this course. We begin with the topic of equivalence relations. 

\subsection{Equivalence Relations}
Let $X$ be a non-empty set. Then, a \textbf{relation} over $X$ is a subset $R$ of $X \times X$. If $(x, y) \in R$, we say that $x$ is $R$-related to $y$ and write $x R y$. 

\bigskip 

So, for these relations, we should think about inequalities equalities, or congruences between integers. 

\bigskip 

Suppose $R$ is a relation over $X$. Then:
\begin{itemize}
    \item $R$ is called \textbf{reflexive} if $\forall x \in X$, $x R x$. That is, every $x \in X$ is related to itself. 
    \item $R$ is called \textbf{symmetric} if $\forall x, y \in X$, $x R y \implies y R x$. In other words, if $x$ is related to $y$, is $y$ related to $x$? 
    \item $R$ is called \textbf{transitive} if $\forall x, y, z \in X$, $x R y$ and $y R z$ implies that $x R z$. 
\end{itemize}

\begin{definition}{Equivalence Relation}{equivRel}
    $R$ is called an \textbf{equivalence relation} if $R$ is reflexive, symmetric, and transitive. 
\end{definition}

\textbf{Remark:} An equivalence relation is essentially an equality with respect to a certain measurement. In life, we often measure things or people with respect to properties (for example, scores or ratings). So, when we want to compare things, we pick a certain property and then, \emph{from that point of view}, determine whether these things are equal. In this regard, equivalence relations are exactly equalities. 

\subsubsection{Example: Relations}
Suppose $X$ and $Y$ are two non-empty sets and $f: X \to Y$ is a function. Let $\sim$ be the following relation over $X$:
\[\forall x_1, x_2 \in X \quad x_1 \sim x_2 \iff f(x_1) = f(x_2)\]
Then, $\sim$ is an equivalence relation\footnote{Another way of interpreting this statement is as follows: $x_1$ is in relation to $x_2$ precisely when $f(x_1) = f(x_2)$. The claim here, then, is that this is an equivalence relation.}.

\begin{mdframed}
    \begin{proof}
        We determine if an relation is an equivalence relation if it satisfies the three properties mentioned above.
        \begin{itemize}
            \item \underline{Reflexivity:}
            \[\forall x \in X, f(x) = f(x) \implies x \sim x\]
    
            \item \underline{Symmetric:}
            \[x_1 \sim x_2 \implies f(x_1) = f(x_2) \implies f(x_2) = f(x_1) \implies x_2 \sim x_1\]
    
            \item \underline{Transitive:}
            We know that:
            \[\forall x_1, x_2 \in X \qquad x_1 \sim x_2 \implies f(x_1) = f(x_2)\]
            We also know that:
            \[\forall x_2, x_3 \in X \qquad x_2 \sim x_3 \implies f(x_2) = f(x_3)\]
            It follows that if $f(x_1) = f(x_2)$ and $f(x_2) = f(x_3)$, then $f(x_1) = f(x_3)$ and thus, $x_1 = x_3$. Namely, $x_1 \sim x_2$ and $x_2 \sim x_3$, then $x_1 \sim x_3$. 
        \end{itemize}
        It follows that this is an equivalence relation. 
    \end{proof}
\end{mdframed}

\subsection{Equivalence Relation Partitions}
Recall that $P$ is called a \textbf{partition} of a non-empty set $X$ if:
\begin{itemize}
    \item \underline{Subsets:} $P$ consists of non-empty subsets of $X$. 
    \item \underline{Disjointness:} $A, B \in P$ and $A \neq B \implies A \cap B = \emptyset$. In other words, the subsets are disjoint. 
    \item \underline{Covering:} $\forall x \in X$, $\exists A \in P$ such that $x \in A$. In other words, every element in $X$ will be in one of the subsets. Alternatively, $\bigcup_{A \in P} A = X$. 
\end{itemize}

\textbf{Remark:}
\begin{itemize}
    \item As mentioned, $P$ is a set of sets. For instance, if we have $X = \{1, 2, 3\}$, one possible $P$ is $P = \{\{1\}, \{2, 3\}\}$.
    \item Below is a visual diagram of what a partition may look like.
\end{itemize}
\begin{center}
    \includegraphics[scale=0.4]{img/partition.PNG}
\end{center}

Suppose $P$ is a partition of $X$. Then, we can get a classification function from $X$ to $P$:
\[X \to P\]
\[x \mapsto [x]_P\]
Here, $[x]_P$ is the unique element of $P$ which contains $x$. In other words, if we refer to the above diagram, we can think of $[x]_P$, a set, as one of the sets $A_1$, $A_2$, $A_3$, $A_4$, or $A_5$ which contains $x$. So, we can think of this function as saying that every $x \in X$ belongs to one of the sets $[x]_P$. 

\bigskip 

Notice that, because of the \textbf{covering} condition, $x$ is contained in some element of $P$; additionally, because of the \textbf{disjointness} condition, $x$ is in an unique element of $P$ (i.e. it is in \underline{one} of the sets which is in $P$). So, it follows that the function is well-defined. 

\bigskip 

By the previous example, $x \sim_P y \iff [x]_P = [y]_P$ is an equivalence relation. So, we obtain the following lemma. 
\begin{lemma}{}{}
    Suppose $P$ is a partition of a non-empty set $X$. For $x, y \in X$, $x \sim y$ if $x$ and $y$ are in the same element of $P$. Then, $\sim$ is an equivalence relation.
\end{lemma}
\textbf{Remark:} Essentially, what this lemma is saying is that if $x \sim y$, then both $x$ and $y$ are in the same set which is in $P$. In other words, if we refer to the above diagram again, we can think of this situation as saying that both $x$ and $y$ are in \underline{one} of $A_1$, $A_2$, $A_3$, $A_4$, or $A_5$. The diagram below complements the proof.

\begin{center}
    \includegraphics[scale=0.30]{img/partition_x.PNG}
\end{center}

\begin{mdframed}
    \begin{proof}
        For $x \in X$, let $[x]_P$ to be the unique element of $P$ which contains $x$. So, $x \mapsto [x]_P$ is a function from $X \to P$. By the previous example, $x \sim y \iff [x]_p = [y]_p$ is an equivalence relation over $X$. Notice that this means $x \sim y$ exactly when $x$ and $y$ are in the same element of $P$. 
    \end{proof}
\end{mdframed}

\subsection{Equivalence Relation Classes}
Now, suppose that $\sim$ is the equivalence relation over a non-empty set $X$, we can partition $X$ with respect to $\sim$. 

\bigskip 

For $x \in X$, we let $[x] = \{y \in X \mid y \sim x\}$ (all the elements that are $\sim$-related to $x$).\footnote{So, it's obvious that $[x] \subseteq X$.} We call $[x]$ the \textbf{equivalence class of $x$ with respect to $\sim$}. When $x \sim y$, we can say that $x$ is equivalent to $y$ with respect to $\sim$. 

\begin{proposition}
    Suppose $\sim$ is an equivalence relation over a non-empty set $X$. Then, $\{[x] \mid x \in X\}$ is a partition of $X$. 
\end{proposition}
This proposition is essentially asking us to show the following properties: 
\begin{itemize}
    \item Covering: Every element of this set belongs to one of these equivalence classes. 
    \item Disjointness: If we pick two equivalence classes, they do not intersect.
\end{itemize}
The following lemma follows from this proposition.
\begin{lemma}{}{}
    \[x \sim y \iff [x] = [y]\]
\end{lemma}
\begin{mdframed}
    \begin{proof}
        We want to show that $[x] = [y] \implies x \sim y$. Recall that the equivalence class of $x$ ($[x]$) and the equivalence class of $y$ ($[y]$) are \emph{sets} and, in particular, we know that $[x]$ consists of all elements that are related to $x$, including $x$. Since $\sim$ is reflexive, we know that:
        \[x \sim x \implies x \in [x]\]
        But, since $[x] = [y]$, then it follows that $x \in [y] \implies x \sim y$. Thus, $[x] = [y] \implies x \sim y$. 
        
        \bigskip 
    
        To show that $x \sim y \implies [x] = [y]$, we need to show equality of sets $[x] = [y]$. This means that it is necessary and sufficient to prove $[x] \subseteq [y]$ and $[y] \subseteq [x]$.
        \begin{itemize}
            \item To prove $[x] \subseteq [y]$, we let $z \in [x]$. This means that $z \sim x$. However, since $x \sim y$, by transitivity, it follows that $y \sim z$, which implies that $z \in [y]$. Hence, $[x] \subseteq [y]$. 
            \item We note that $x \sim y \implies y \sim x$ by symmetry. Therefore, by the first bullet point, $[y] \subseteq [x]$.
        \end{itemize} 
        So, it follows that $x \sim y \implies [x] = [y]$. 
    \end{proof}
\end{mdframed}

Now that we proved the lemma, we can now prove the proposition. 
\begin{mdframed}
    \begin{proof}
        As mentioned, we need to show that the covering and disjointness properties exist in this partition.
        \begin{itemize}
            \item \underline{Covering:} $\forall x \in X$, we know that $x \sim x$ by the reflexive property (since $\sim$ is an equivalence relation). Thus, it follows that $x \in [x]$. This means that $x$ is related to $x$ and $x$ is an equivalence class of $x$, so every element in $X$ belongs to one of the equivalence classes. This implies that the $[x]$ sets are non-empty subsets and cover $X$.
            \item \underline{Disjointness:} Suppose $z \in [x] \cap [y]$ (both equivalence classes are not disjoint). We need to show that they are equal. We know that:
            \[z \in [x] \cap [y] \implies z \in [x] \implies z \sim x \implies [z] = [x]\]
            \[z \in [x] \cap [y] \implies z \in [y] \implies z \sim y \implies [z] = [y]\]
            Where the last two steps came from the lemma. Then, putting these two together, we have:
            \[[z] = [x] \text{ and } [z] = [y] \implies [x] = [y]\]
            We showed that $[x] \cap [y] \neq \emptyset \implies [x] = [y]$, the contrapositive of the disjointness property. 
        \end{itemize}
        Thus, the proof is complete. 
    \end{proof}
\end{mdframed}

\subsection{Summary}
The following diagram provides a brief summary of what we've learned\footnote{This diagram was taken from Professor Alireza Salehi Golsefidy's notes.}
\begin{center}
    \includegraphics[scale=0.9]{img/equiv_relation.PNG}
\end{center}








% --------------------------------------------------------- %
%                   NEW SECTION                             %
% --------------------------------------------------------- %
\newpage 
\section{Review: Congruences, Long Division, Modulo}
In this section, we discuss congruences, long division, and modulo. 

\subsection{Congruence}
The set of integers is denoted by $\Z$. For $a, b \in \Z$, we say that $a$ divides $b$ and write $a | b$ if $b = ak$ for some $k \in \Z$. Suppose $n$ is a non-zero integer. Then, we say that $a$ is \textbf{congruent} to $b$ modulo $n$ and write one of the following if $n | (a - b)$:
\[a \equiv b \Mod{n}\]
\[a \overset{n}{\equiv} b\]
One way to think of this is through a clock. A clock has $n$ numbers (usually 12 numbers). Then, $a$ and $b$ will be on the same spot. For instance, suppose we have 9PM (denoted by the 21st hour). Then, we know that:
\[21 \equiv \boxed{9} \Mod{12}\]
In other words, the hour hand for 9PM will be in the same position as 9AM. 

\subsection{Congruence and Equivalence Relations}
\begin{lemma}{}{}
    $\overset{n}{\equiv}$ is an equivalence relation over $\Z$. 
\end{lemma}
\begin{mdframed}
    \begin{proof}
        Recall that something is an equivalence relation if it satisfies the three properties mentioned in definition \ref{def:equivRel}. So, we need to show that $\overset{n}{\equiv}$ satisfies these. 
        \begin{itemize}
            \item \underline{Reflexive}: For every $a \in \Z$, we know that $a - a = 0$ is a multiple of $n$ as $n \times 0 = 0$. Hence, $a \overset{n}{\equiv} a$. 
            \item \underline{Symmetric}: We have that: 
            \begin{equation*}
                \begin{aligned}
                    a \overset{n}{\equiv} b &\implies n | (a - b) \\ 
                        &\implies \exists k \in \Z, a - b = nk \\ 
                        &\implies b - a = n\underbrace{(-k)}_{\text{In } \Z} \\ 
                        &\implies b \overset{n}{\equiv} a
                \end{aligned}
            \end{equation*}
    
            \item \underline{Transitive:} We know that:
            \[a \overset{n}{\equiv} b \implies n | (a - b) \implies \exists k \in Z, a - b = nk\]
            We also know that:
            \[b \overset{n}{\equiv} c \implies n | (b - c) \implies \exists l \in \Z, b - c = nl\]
            Combining the statements, we now have:
            \[(a - b) + (b - c) = nk + nl \implies a - c = n\underbrace{(k + l)}_{\text{In } \Z} \implies a \overset{n}{\equiv} c\]
        \end{itemize}
        Thus, $\overset{n}{\equiv}$ is an equivalence class. 
    \end{proof}
\end{mdframed}

\subsection{Congruence and Partitions}
As we have seen earlier, every equivalence relation gives us a \textbf{partition} and an \textbf{equality function}. For $a \in \Z$, the equivalence class of $a$ with respect to $\overset{n}{\equiv}$ is called the \textbf{mod-$n$ residue class of $a$} and is denoted by $[a]_n$. By the results that we proved for equivalence relations, we have that:
\begin{itemize}
    \item $\{[a]_n \mid a \in \Z\}$ is a partition of $\Z$; and 
    \item $a \overset{n}{\equiv} b \iff [a]_n = [b]_n$ 
\end{itemize}

The partition $\{[a]_n \mid a \in \Z\}$ is denoted by $\Z_n$ and it is called \textbf{the set of integers modulo $n$}. Notice that:
\begin{equation*}
    \begin{aligned}
        b \in [a]_n &\iff b \overset{n}{\equiv} a \\ 
            &\iff n | (b - a) \\ 
            &\iff \exists k \in \Z, b - a = nk \\ 
            &\iff \exists k \in \Z, b = a + nk \\ 
            &\iff b \in \{a + nk \mid k \in \Z\} & \text{Arithmetic Progression}
    \end{aligned}
\end{equation*} 

To understand the set $\Z_n$ better, we recall the well-ordering principle and the long division property of integers. One of the important properties of positive integers is the \textbf{well-ordering principle}. This principle can be viewed as an axiom that we assume $\Z$ has. 

\subsubsection{Well-Ordering Principle}
Every non-empty subset of the set $\Z_{\geq 0}$ of non-negative integers has a minimum. Using the well-ordering principle, we can prove the division algorithm. 

\subsubsection{The Division Algorithm}
For every $a \in \Z$, $b \in \Z - \{0\}$, there is a unique pair $(q, r)$ of integers such that:
\[a = bq + r \qquad 0 \leq r \leq |b|\]
To show that this is the case, consider the following diagram:
\begin{center}
    \includegraphics[scale=0.4]{img/mod_div.PNG}
\end{center}
For positive integers $a$ and $b$, we can keep ``cutting off'' the $b \times b$ squares until we are left with either a rectangle smaller than $b \times b$ (i.e. if $r > 0$), or nothing at all (i.e. $r = 0$). In this sense, the number of $b \times b$ squares that we cut off is denoted by $q$ (or $k$ soon). Then, it follows that $r$ is the smallest non-negative integer in the arithmetic progression $a - bk$.

\bigskip 

Now, we want to formalize this argument. We begin by defining: 
\[\Sigma := [a]_b \cap \Z_{\geq 0} = \{a + bk \mid k \in \Z, a + bk \geq 0\}\]
Here, we denote $\Sigma$ as a variable, not a summation. We want to use the well-ordering principle on the set $\Sigma$ to show that it has a minimium (more specifically, that $\Sigma$ is not empty). Our first claim is:
\[\Sigma \text{ is not empty.}\]
\begin{mdframed}
    \begin{proof}
        Since $b \neq 0$, we know that $|b| \geq 1$. Hence, $|b||a| \geq |a|$. Therefore:
        \[\underbrace{|b|(|a| + 1) + a}_{a \pm b(|a| + 1)} \geq |a| + |b| + a \geq |b| > 0\]
        It follows that $[a]_b$ has a positive integer. 
    \end{proof}
\end{mdframed}
By the above claim and the well-ordering principle, $\Sigma$ has a minimum. Now, suppose $r$ is the minimum of $\Sigma$. Then, $r = a - bq$ for some $q \in \Z$. Our second claim is that $r < |b|$. 
\begin{mdframed}
    \begin{proof}
        Suppose to the contrary that $r \geq |b|$. Then, $r - |b| \geq 0$ and:
        \[r - |b| = a - bq - |b| = a - b(q \pm 1)\]
        In the above statement, we know that $r = a - bq$, which is in the arithmetic progression. When we subtract $|b|$, we are still in this arithmetic progression since all we're doing is adding or subtracting $b$; in either case, we're not leaving the arithmetic progression. So, it follows that $r - |b|$ is still in this arithmetic progression.
    
        \bigskip 
    
        Hence, $r - |b| \in \Sigma$ (recall that $\Sigma$ consists of non-negative integers). This is a contradiction as $r - |b|$ is smaller than the minimum $r$ of $\Sigma$. 
    \end{proof}
\end{mdframed}

By this claim, we obtain the existence of the pair $(q, r)$ with:
\begin{itemize}
    \item $a = bq + r$
    \item $0 \leq r < |b|$
\end{itemize}
Now, we want to prove the uniqueness.
\begin{mdframed}
    \begin{proof}
        Suppose the pairs $(q, r)$ and $(q', r')$ satisfy the desired properties; that means:
        \begin{enumerate}[(a)]
            \item $a = bq + r = bq' + r'$
            \item $0 \leq r, r' \leq |b|$
        \end{enumerate}
        Then, $b(q - q') = (r' - r)$. Notice that:
        \[0 \leq r \implies r' - r \leq r' < |b|\]
        \[0 \leq r' \implies r' - r \geq -r > -|b|\]
        Then, we have that:
        \[|r' - r| < |b|\]
        And we obtain that:
        \[|r' - r| = |b|(q - q') = |b| |q - q'|\]
        And, thus:
        \[|b||q - q'| < |b|\]
        This implies that $|q - q'| < 1$. Since $|q - q'|$ is a non-negative integer less than 1, it is 0. Thus, $q = q'$. It follows that:
        \[r' - r = b(q - q') = 0\]
        This implies that $r = r'$. Thus, this shows the uniqueness. 
    \end{proof}
\end{mdframed}

Suppose the pair $(q, r)$ is given in the long division algorithm. Then, $q$ is called the \textbf{quotient} of $a$ divided by $b$ and $r$ is called the remainder of $a$ divided by $b$. Using the long division algorithm, we obtain that $\Z_n$ has $n$ elements. 

\begin{proposition}
    Suppose $n$ is an integer more than 1. Then:
    \[\Z_n = \{[0]_n, [1]_n, \dots, [n - 1]_n\} \quad |\Z_n| = n\]
\end{proposition}
\begin{mdframed}
    \begin{proof}
        For every $a \in \Z$, by the long division algorithm, there are integers $q$ and $r$ such that $a = nq + r$ and $0 \leq r < n$. 
    \end{proof}
\end{mdframed}


% \overset{n}{\equiv}




% --------------------------------------------------------- %
%                   NEW SECTION                             %
% --------------------------------------------------------- %
\newpage 
\section{Introduction to Binary Operations and Group Theory}
We want to explore the idea behind \emph{algebraic structures}. In particular, we want to explore these structures in more detail compared to earlier courses (either in past college or high school algebra classes). 

\bigskip 

To do this, we need to think about \emph{what} algebra really is. We might think about solving equations like $x^2 + 3x + 5 = 0$ for $x$. In particular, what is really happening here?

\bigskip 

Well, there are a couple of operations going on. Specifically, we have \emph{addition} and \emph{multiplication}. 
\[x \times x + 3 \times x + 5 = 0\]
We now want to examine these operations. Both of these operations $(+, \times)$ take in \underline{two numbers} and output \underline{one number}. The question we might have, then, is: how can we can generalize these operations?

\subsection{Binary Operations}
A \textbf{binary operation} is a way of taking in two values and outputting one value. Of course, we might now ask: what can these values be? These values can come from any specific set. 

\bigskip 

For example, we can consider addition over the integers ($\Z$). The sum of two integers is an integer. Similarly, we could consider multiplication over the integers. Again, the product of two integers is an integer. We could also consider multiplication or addition over the real, rational, or complex numbers. 

\bigskip 

The idea is that whatever ``type'' we give our binary operation, we will get that same ``type'' for our output. To formalize this, we have the following definition:  
\begin{definition}{Binary Operation}{}
    A binary operation $*$ over a set $S$ is a function mapping $f: S \times S \to S$. For each $(a, b) \in S \times S$, we will denote the element $*((a, b))$ of $S$ by $a * b$.\footnote{As a side note, in this class, $a * b$ is equivalent to $f(a, b)$ and $b * a$ is equivalent to $f(b, a)$.}
\end{definition}
%Recall that the Cartesian Product of two sets $S_1$ and $S_2$ is denoted by $S_1 \times S_2$ and is defined by:
%\[S_1 \times S_2 = \{(s_1, s_2) \mid s_1 \in S_1, s_2 \in S_2\}\]
%Here, we note that $(a, b) \neq (b, a)$ unless $a = b$. 
%\bigskip 

We also introduce the notion of closure, which will be used later. 
\begin{definition}{Closure}{}
    Let $*$ be a binary operation on $S$ and let $H$ be a subset of $S$. The subset $H$ is \textbf{closed under} $*$ if for all $a, b \in H$ we also have $a * b \in H$. In this case, the binary operation on $H$ given by restricting $*$ to $H$ is the \textbf{induced operation} of $*$ on $H$.
\end{definition}

Anything that is ``like'' addition or multiplication is probably a binary operation. For example, let's consider \textbf{matrices}.
\begin{itemize}
    \item Addition of matrices of a fixed dimension. More specifically, the set of $n \times m$ matrices (here, $n$ and $m$ are fixed positive integers) over the integers, rationals, reals, or complex numbers under matrix addition is a binary operation.
    \[
        \begin{bmatrix}
            a_{11} & a_{12} & a_{13} \\ 
            a_{21} & a_{22} & a_{23}
        \end{bmatrix} + \begin{bmatrix}
            b_{11} & b_{12} & b_{13} \\ 
            b_{21} & b_{22} & b_{23}
        \end{bmatrix} = \begin{bmatrix}
            a_{11} + b_{11} & a_{12} + b_{12} & a_{13} + b_{13} \\ 
            a_{21} + b_{21} & a_{22} + b_{22} & a_{23} + b_{23}
        \end{bmatrix}
    \]

    \item Multiplication of matrices of a fixed dimension. More specifically, the set of $n \times n$ matrices (square matrices). We could also just multiply a $(n \times m)$ matrix by a $(k \times l)$ matrix assuming $m = k$ (otherwise, multiplying these two matrices will result in undefined behavior). 
\end{itemize}

So far, we considered binary operations on infinite sets in which we need some sort of formula to describe (e.g. $f_{\cup}(A, B) = A \cup B$). Now, if we have a finite set, we could define a binary operation exhaustively by just saying what the binary operation does on every pair of entries.

\bigskip 

For example, given the set $S = \{a, b, c, d, e\}$. We can define a binary operation on $S$ with the below (random) table (first entry on left side, second entry on top side). 
\begin{center}
    \begin{tabular}{c c c c c c}
            & $a$ & $b$ & $c$ & $d$ & $e$ \\ 
        \hline 
        $a$ & $a$ & $c$ & $d$ & $d$ & $e$ \\ 
        $b$ & $b$ & $c$ & $c$ & $b$ & $a$ \\ 
        $c$ & $d$ & $e$ & $e$ & $b$ & $b$ \\ 
        $d$ & $a$ & $a$ & $a$ & $c$ & $a$ \\ 
        $e$ & $b$ & $b$ & $c$ & $c$ & $d$
    \end{tabular}
\end{center}
Denote the binary operation to be $\#$.
\begin{itemize}
    \item What is $c \# d$? The answer is $b$. 
    \item What is $e \# ((a \# b) \# c)$? The answer is $d$.
    \item Suppose we have $X \# a = a$. What is $X$? The answer is $X = a, d$.
\end{itemize}

\subsection{Properties of Binary Operations}
What properties could binary operations have?\footnote{In this course, we will consider binary operations with all the properties excluding commutativity.}

\begin{itemize}
    \item \textbf{Commutativity:} A binary operation is commutative if the order of the two inputs does not matter. For example, if $f$ is a function corresponding to a binary operation, then:
    \[f(a, b) = f(b, a) \quad \forall a, b \in S\] 
    More commonly:
    \[a * b = b * a \quad \forall a, b \in S\]

    For example, addition or multiplication of numbers is commutative. Unions and intersections of sets is also commutative. \emph{However}, matrix multiplication is \emph{not} commutative. Our example above is also not commutative. 

    \item \textbf{Associativity:} A binary operation is associative if the order of applying the operation (in a string) does not matter. Specifically:
    \[(a * b) * c = a * (b * c) \quad \forall a, b, c \in S\]
    Which means that we can write $a * b * c$ without ambiguity.
    
    \bigskip 

    For example, addition or multiplication of numbers is associative. Addition or multiplication of matrices is also associative. Our example above is not associative. 

    \item \textbf{Identity:} A binary operation has a two-sided identity element and a two-sided inverse for every element. 
    
    \bigskip 
    
    More specifically, we say that $\epsilon$ is a left identity if $f(\epsilon, s) = s$ for all $s \in S$. $\epsilon$ is a right identity if $f(s, \epsilon) = s$ for all $s \in S$. Then, $\epsilon$ is a two-sided identity if it is both a left identity and right identity.  
    
    \bigskip 

    For example, 0 is a two-sided identity for addition and 1 is a two-sided identity for multiplication. For matrix addition, the zero-matrix is a two-sided identity. For matrix multiplication, the matrix with ones on the diagonal and zeros everywhere else is the identity element. In our example above, $\#$ does not have a left or right identity. 

    \bigskip 

    Given a two-sided identity, we can also consider the idea of an inverse. In addition, this is the negative/negation. In multiplication, this is the reciprocal. The additive inverse of $x$ is $-x$. The multiplicative inverse of $x$ is $\frac{1}{x}$ (for all $x \neq 0$). 
    
    \bigskip 
    
    For a general binary operation $f: S \times S \to S$ with a two-sided identity $\epsilon$, an element $s \in S$ has a two-sided inverse if there exists an element $t \in S$ such that:
    \[\underbrace{f(s, t)}_{\text{Right Inverse}} = \overbrace{f(t, s)}^{\text{Left Inverse}} = \epsilon\] 

    So, a property for binary operations would be for every element to have a \underline{two-sided inverse}, which requires a \underline{two-sided identity element}.
\end{itemize}

\textbf{Remark:} Commutativity does not imply associativity.

\subsection{Groups}
Of course, the properties of binary operations that were discussed just now are very much applicable in something called \textbf{groups}. Simply put, we can say that a group is a set combined with an operation. However, it's a little more complicated than that. The following definition will make that clearer:
\begin{definition}{Group}{}
    A group is a set $G$, closed under a binary operation $*$, satisfying the three properties:
    \begin{enumerate}
        \item \underline{Associativity}: For all $a, b, c \in G$, we have:
        \[(a * b) * c = a * (b * c)\]

        \item \underline{Identity/Neutral Element:} There is an element $\epsilon \in G$ such that for all $x \in G$:
        \[\epsilon * x = x * e = x\]

        \item \underline{Inverse:} Corresponding to each $a \in G$, there is an element $a' \in G$ such that:
        \[a * a' = a' * a = \epsilon\]
        It is also common to denote the inverse of $a$ as $a^{-1}$ instead of $a'$.
    \end{enumerate}
\end{definition}
\textbf{Remark:}
\begin{itemize}
    \item Notationally, this can be represented by $(G, *)$ or $\langle G, * \rangle$. This is saying that we are pairing a set with a binary operation. 
    \item With regards to how we write the inverse, unless otherwise mentioned, I will use $a'$ and $a^{-1}$ interchangeably.
\end{itemize}

\begin{note*}{}{}
    The two most common groups are additive and multiplicative groups. Thus, for some $h \in G$, where $(G, *)$ is a group, it is important to mention what their inverses and identity elements are. As mentioned in the previous section:
    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline 
            \textbf{Group} & \textbf{Inverse} & \textbf{Identity} \\ 
            \hline 
            Multiplicative $(G, \times)$ & $h^{-1} = h' = \frac{1}{h}$ & $\epsilon = 1$ \\ 
            Addition $(G, +)$ & $h^{-1} = h' = -h$ & $\epsilon = 0$ \\ 
            \hline 
        \end{tabular}
    \end{center}
    We will discuss these more in the examples. 

    \bigskip 

    For any other group, the inverse and identity element depends on how the group and its binary operation is defined. Refer to the definition of a group.
\end{note*}

\subsection{Basic Properties of Groups}
Suppose $(G, *)$ is a group. Then, we note the following properties of groups. 

\subsubsection{Uniqueness of the Identity.} 
Could we have two unique two-sided identities in $G$? The answer is \underline{no}. The proof is as follows. 

\begin{mdframed}
    \begin{proof}
        Assume by contradiction that we had $\epsilon_1$ and $\epsilon_2$, both of which are unique two-sided identity elements. Then, we know that $\epsilon_1 * \epsilon_2 = \epsilon_2$ since $\epsilon_1$ is an identity. But, since $\epsilon_2$ is also an identity, then $\epsilon_1 * \epsilon_2 = \epsilon1$. So, it follows that $\epsilon_1$ and $\epsilon_2$ are not unique; in other words, $\epsilon_1 = \epsilon_2$. 
    \end{proof}
\end{mdframed}

\subsubsection{Uniqueness of Inverses.}

If $g_1$, $g_2$ are both inverses of some element $h$, then\footnote{Here, we denote $g_1$ as the left-inverse and $g_2$ is the right-inverse.}:
\[g_1 * h = h * g_2 = \epsilon\]
Additionally, we know that:
\[g_1 * (h * g_2) = g_1 * \epsilon = g_1\]
\[(g_1 * h) * g_2 = \epsilon * g_2 = g_2\]
And so it follows that $g_1 = g_2$, thus $h$ will have a unique inverse. To be more concrete, we have the proof. 
\begin{mdframed}
    \begin{proof}
        We note that $g_1 * h = \epsilon$ and $h * g_2 = \epsilon$. Then:
        \begin{equation*}
            \begin{aligned}
                g_1 &= g_1 * \epsilon && \epsilon \text{ is the identity element.} \\ 
                    &= g_1 * (h * g_2) \\ 
                    &= (g_1 * h) * g_2 && \text{Associativity} \\ 
                    &= \epsilon * g_2 \\ 
                    &= g_2 && \epsilon \text{ is the identity element.}
            \end{aligned}
        \end{equation*}
        So, it follows that $g_1 = g_2$. Thus, an element $h$ will have a unique inverse. 
    \end{proof}
\end{mdframed}

\subsubsection{Cancellation.}
Suppose we have the expression $g * a = g * b$. This implies that $a = b$. Similarly, the expression $a * g = b * g$ can be simplified to $a = b$. 

\begin{mdframed}
    \begin{proof}
        From the definition of a group, we know that an inverse exists for every element in $G$. Let $g^{-1}$ be the inverse of $g$. Then:
        \begin{equation*}
            \begin{aligned}
                g * a = g * b &\implies g^{-1} * (g * a) = g^{-1} * (g * b) \\
                    &\implies (g^{-1} * g) * a = (g^{-1} * g) * b && \text{Associativity (Prop. 1)} \\
                    &\implies \epsilon * a = \epsilon * b && \text{Definition of Inverse (Prop. 3)} \\  
                    &\implies a = b && \text{Definition of Identity (Prop. 2)}
            \end{aligned}
        \end{equation*}
        The other way is similar. 
    \end{proof}
\end{mdframed}

\textbf{Remark:} Although $g * a = g * b$, $g * a \neq b * g$ ($g * a$ is not necessarily equal to $b * g$). 

\subsubsection{Inverse of Operation of Two Elements.}

\begin{lemma}{}{}
    Suppose $(G, *)$ is a group. Then, for every $g, h \in G$, we have: 
    \[(g * h)^{-1} = h^{-1} * g^{-1}\]
\end{lemma}

\begin{mdframed}
    \begin{proof}
        Since the inverse of an element is unique, it is enough to check that: 
        \[(g * h) * (h^{-1} * g^{-1}) = (h^{-1} * g^{-1}) * (g * h) = \epsilon\]
        So: 
        \begin{equation*}
            \begin{aligned}
                (g * h) * (h^{-1} * g^{-1}) &= g * (h * h^{-1}) * g^{-1} && \text{Associativity (Prop. 1)} \\ 
                    &= g * \epsilon * g^{-1} && \text{Definition of Inverse (Prop. 3)} \\ 
                    &= (g * \epsilon) * g^{-1} && \text{Associativity (Prop. 1)} \\ 
                    &= g * g^{-1} && \text{Definition of Identity (Prop. 2)} \\ 
                    &= \epsilon && \text{Identity Element}
            \end{aligned}
        \end{equation*}
        Similarly: 
        \begin{equation*}
            \begin{aligned}
                (h^{-1} * g^{-1}) * (g * h) &= h^{-1} * (g^{-1} * g) * h && \text{Associativity (Prop. 1)} \\ 
                    &= h^{-1} * \epsilon * h && \text{Definition of Inverse (Prop. 3)} \\ 
                    &= (h^{-1} * \epsilon) * h && \text{Associativity (Prop. 1)} \\ 
                    &= h^{-1} * h && \text{Definition of Identity (Prop. 2)} \\ 
                    &= \epsilon && \text{Identity Element}
            \end{aligned}
        \end{equation*}
        So, the proof is complete. 
    \end{proof}
\end{mdframed}

\subsubsection{Inverse of an Inverse.}
We should note that, despite using the $-1$ superscript to denote a multiplicative inverse, this applies to any valid binary operation under a group. 

\begin{lemma}{}{}
    For every $g \in G$, $(g^{-1})^{-1} = g$. 
\end{lemma}

\begin{mdframed}
    \begin{proof}
        We have that $g^{-1} * g = \epsilon$. Multiplying both sides by $(g^{-1})^{-1}$ from the left, we now have: 
        \[((g^{-1})^{-1} * g^{-1}) * g = (g^{-1})^{-1} * \epsilon = (g^{-1})^{-1}\]
        Hence, $\epsilon * g = (g^{-1})^{-1}$ and so $g = (g^{-1})^{-1}$.     
    \end{proof}
\end{mdframed}

\subsection{Examples and Non-Examples}
Here, we briefly talk about some examples and non-examples of groups. 
\subsubsection{Example: Addition}
For example, the integers under addition are a group. Notationally, this is represented by $(\Z, +)$. 
\begin{itemize}
    \item It's obvious that addition is associative. That is:
    \[(a + b) + c = a + (b + c) = a + b + c\]

    \item The identity element is 0 (we note that $0 \in \Z$). This is because:
    \[0 + x = x + 0 = x\]

    \item The inverse is $-x$. This is because:
    \[x + (-x) = (-x) + x = 0\]
\end{itemize}
We also know that the reals, rationals, or complex numbers under addition are also groups. Notationally, this is represented by $(\R, +)$, $(\Q, +)$, or $(\C, +)$, respectively. 

\subsubsection{Example: Multiplication}
Let's now consider multiplication. In particular, multiplication does give a binary operation over $\Z$, $\Q$, $\R$, and $\C$. It's obvious that this is associative and 1 is the two-sided identity element. However, what about the inverse? 
\begin{itemize}
    \item If we try to take the integers under multiplication as a group, then we'll run into problems. This is because the multiplicative inverse of every \underline{integer} except $\pm 1$ is not an integer. For example, if we tried 2, then the multiplicative inverse of 2 is $\frac{1}{2}$. However, $\frac{1}{2} \notin \Z$. 
    
    \item Rational numbers are closer. For instance, $\left(\frac{a}{b}\right)^{-1} = \frac{b}{a}$. However, this is only defined if $a \neq 0$. The solution is to remove 0. Define $\Q^*$ to be the non-zero rational numbers (i.e. $\Q^* = \Q - \{0\}$). Then, $(\Q^*, \times)$ is a group. Similarly, we can make $\R$ and $\C$ groups under multiplication by removing 0. 
    
    \bigskip 

    We note that this change does not affect the closure property because we can only achieve $a \times b = 0$ if and only if $a = 0$ or $b = 0$. Since $a \notin \R - \{0\}$ and $b \notin \R - \{0\}$ (or $\Q$ or $\C$), then we are still closed and our binary operation is still well-defined. 
\end{itemize}

\subsubsection{Non-Example: Addition and Multiplication}
We mentioned that $(\Q - \{0\}, \times)$, $(\R - \{0\}, \times)$, and $(\C - \{0\}, \times)$ are groups. However, we note that $(\Z - \{0\}, \times)$ and $(\Z_{\geq 0}, +)$ are \emph{not} groups. 
\begin{itemize}
    \item We already briefly explained why $\Z$ under multiplication is not a group. The same idea applies even if we do not include 0; that is, $\Z - \{0\}$ is not a group. We know that $\Z - \{0\}$ has a unique identity element under $\times$; this element is 1. This is the case because, if $\epsilon$ is the identity element of $\Z - \{0\}$ under $\times$, then by definition: 
    \[\epsilon \times x = x \times \epsilon = x\]
    Which implies that $\epsilon = 1$. We also know that $2 \in \Z - \{0\}$. However, 2 does not have an inverse in $\Z - \{0\}$. To show this, we prove by contradition. If 2 has an inverse in $\Z - \{0\}$, then by definition it follows that for some $a' \in \Z - \{0\}$:
    \[2 \times a' = a' \times 2 = \epsilon\]
    But, since we know that $\epsilon = 1$, it follows that:
    \[2 \times a'= 1\]
    But, as the only solution to this is $\frac{1}{2}$, we know that $\frac{1}{2} \notin \Z - \{0\}$. Thus, this is a contradiction. Thus, $\Z - \{0\}$ under multiplication is not a group. 

    \item We know that $\Z_{\geq 0}$ has a unique identity element under addition and that is 0. This is because if $\epsilon$ is a unique element of $(\Z_{\geq 0}, +)$, then by definition, we know that: 
    \[\epsilon + x = x + \epsilon = x\]
    It is obvious that $\epsilon = 0$. Now, we want to show that 1 does not have an inverse with respect to addition in $\Z_{\geq 0}$. We'll prove this by contradiction. Suppose 1 does have an inverse. Recall that if 1 does have an inverse, then there is an $x \in \Z_{\geq 0}$ such that for some $a' \in \Z_{\geq 0}$:
    \[a' + 1 = 1 + a' = \epsilon\]
    But, as $\epsilon = 0$, it follows that: 
    \[a' + 1 = 0 \iff a' = -1\]
    However, we note that $-1 \notin \Z_{\geq 0}$ so this is a contradiction. Thus, $\Z_{\geq 0}$ under addition is not a group.
\end{itemize}

\subsubsection{Example: Addition and Modular Arithmetic}
More examples of groups come from modular arithmetic. For instance, consider addition modulo $n$ for some integer $n$. The set that we're going to be working with is \emph{equivalence classes} modulo $n$. Recall that\footnote{In general, $[a]_n = \overline{a} = \{x \in \Z \mid x \equiv a \mod{n}\}$}:
\begin{center}
    \[[0]_n = \overline{0} = \{\dots, -3n, -2n, -n, 0, n, 2n, 3n, \dots\}\]
    \[[1]_n = \overline{1} = \{\dots, 1 - 3n, 1 - 2n, 1 - n, 1, 1 + n, 1 + 2n, 1 + 3n, \dots\}\]
    \[[2]_n = \overline{2} = \{\dots, 2 - 3n, 2 - 2n, 2 - n, 2, 2 + n, 2 + 2n, 2 + 3n, \dots\}\]
    \vdots 
    \[[n - 1]_n = \overline{n - 1} = \{\dots, -n - 1, -1, n - 1, 2n - 1, \dots\}\]
\end{center}
We define our binary operation on the set of equivalence classes $\{\overline{0}, \overline{1}, \overline{2}, \dots, \overline{n - 1}\}$. What properties does this have? We know that modulo $n$ addition on these equivalence classes is:
\begin{itemize}
    \item Associative (and commutative).
    \item It has identity $\overline{0}$. If you have equivalence class $\overline{k}$ (for $k \geq 1$), then the inverse if $\overline{n - k}$ for $1 \leq k \leq n - 1$ so that $1 \leq n - k \leq n - 1$. We note that:
    \[\overline{k} + \overline{n - k} = \overline{0} \Mod{n}\]
    \[k + (n - k) = n \in \overline{0}\]
    \item $\overline{0}$ is the inverse of $\overline{0}$. 
\end{itemize}
We denote this group as $(\Z_n, + \Mod{n})$, where $\Z_n = \{\overline{0}, \overline{1}, \dots, \overline{n - 1}\}$. When it is clear, we can drop the lines. 

\subsubsection{Example: Multiplication and Modular Arithmetic}
Could we do multiplication modulo $n$ as a group? Well, multiplication modulo $n$ on the set $\Z_n$ is an associative, commutative, binary operation with identity $\overline{1}$. However, inverses are potentially an issue ($\overline{0}$ specifically will be an issue). Can we fix this by removing all uninvertible elements? 
\begin{itemize}
    \item Yes, but can we characterize uninvertible elements? Well, the greatest common divisor of two integers is a linear combination. This is useful because we can think about $\gcd(k, n)$ where $k \in \{1, \dots, n - 1\}$. Specifically, if $\gcd(k, n) = 1$, then $ak + bn = 1$ implies that $\overline{a} \times \overline{k} = \overline{1} \Mod{n}$. In other words, $k$ is invertible if $\gcd(n, k) = 1$. 
    \item What if $\gcd(k, n) > 1$? If $k$ was invertible under multiplication modulo $n$, then $\overline{a} \overline{k} = \overline{1} \Mod{n}$. But, that would mean that there is a linear combinations of $a$ and $k$ that is equal to 1. Thus, $k$ is invertible under multiplication modulo $n$ if and only if $\gcd(k, n) = 1$. 
\end{itemize}
Let's now consider only taking equivalence classes $\overline{k}$ where $\overline{k}$ is relatively prime for $n$. We will now make the claim that $(U(n), \times \Mod{n})$ is a group. We know that this is associative and commutative, we justified that it has an inverse and an identity element. However, is this still closed? 

\bigskip 

More formally, why do we have closure of the binary operations? We could think about this in several ways. We could give a proof that the product two integers that are relatively prime to $n$ is still relatively prime to $n$, or we can think about this more algebraically: namely, we can justify that the product of two invertible equivalence classes is also invertible. 

\bigskip 

If $k_1, k_2$ are both invertible, then $k_1 k_2$ is also invertible. Why is this the case? Well, $k_1$ and $k_2$ being invertible means that there is some value $k_1^{-1}$ and $k_2^{-1}$. So, let's consider $k_2^{-1} k_1^{-1}$. We know that:
\begin{equation*}
    \begin{aligned}
        (k_1 k_2) \left[k_2^{-1} k_1^{-1}\right] &= k_1 \left[k_2 k_2^{-1}\right] k_1^{-1} \\ 
            &= k_1 \epsilon k_1^{-1} \\ 
            &= k_1 k_1^{-1} \\ 
            &= \epsilon
    \end{aligned}
\end{equation*}

\subsection{Exponents of Elements}
Suppose $(G, *)$ is a group and $g \in G$. For a positive integer $n$, we let: 
\[g^n = \underbrace{g * \dots * g}_{n \text{ times}}\]
For a negative integer $n$, we let: 
\[g^n = \underbrace{(g^{-1}) * \dots * (g^{-1})}_{-n \text{ times}}\]

\begin{lemma}{}{}
    For $n, m \in \Z$, $(g^n)^m = g^{nm}$. 
\end{lemma}

\begin{mdframed}
    \begin{proof}
        We will consider various cases depending on the signs of $m$ and $n$. 
        \begin{itemize}
            \item \underline{Case 1:} Suppose $m$ and $n$ are positive. Then: 
            \[(g^n)^m = \underbrace{g^n * \dots * g^n}_{m \text{ times}} = \underbrace{\overbrace{(g * \dots * g)}^{n \text{ times}} * \dots * \overbrace{(g * \dots * g)}^{n \text{ times}}}_{m \text{ times}} = \underbrace{g * \dots * g}_{mn \text{ times}} = g^{mn}\]
            Here, $g^n$ means we need to multiply $g$ $n$ times. But, since we need to multiply $g^n$ $m$ times, it follows that this is simply $g^{nm}$.  

            \item \underline{Case 2:} Suppose $m$ is positive and $n$ is negative. Then: 
            \[(g^n)^m = \underbrace{g^n * \dots * g^n}_{m \text{ times}} = \underbrace{\overbrace{(g^{-1} * \dots * g^{-1})}^{-n \text{ times}} * \dots * \overbrace{(g^{-1} * \dots * g^{-1})}^{-n \text{ times}}}_{m \text{ times}} = \underbrace{g^{-1} * \dots * g^{-1}}_{-mn \text{ times}} = g^{mn}\]
            Here, we note that $mn < 0$. 

            \item \underline{Case 3:} Suppose $m$ is negative and $n$ is positive. Then: 
            \[(g^n)^m = \underbrace{(g^n)^{-1} * \dots * (g^n)^{-1}}_{-m \text{ times}} = \underbrace{(\overbrace{g * \dots * g}^{n \text{ times}})^{-1} * \dots * (\overbrace{g * \dots * g}^{n \text{ times}})^{-1}}_{-m \text{ times}}\]

            We note that, by the previous lemma, $(\underbrace{g * \dots * g}_{n \text{ times}})^{-1} = \underbrace{g^{-1} * \dots * g^{-1}}_{n \text{ times}}$. Hence: 
            \[(g^n)^m = \underbrace{(\overbrace{g^{-1} * \dots * g^{-1}}^{n \text{ times}}) * \dots * (\overbrace{g^{-1} * \dots * g^{-1}}^{n \text{ times}})}_{-m \text{ times}} = \underbrace{g^{-1} * \dots * g^{-1}}_{-mn \text{ times}} = g^{mn}\]
            Here, we note that $mn < 0$.  

            \item \underline{Case 4:} Suppose $m$ and $n$ are negative. Since it is easier to work with positive numbers, let $m = -r$ and $n = -s$ where $r, s > 0$. Then, we have to show that $(g^{-r})^{-s} = g^{rs}$. By definition, we know that $g^{-r} = \underbrace{g^{-1} * \dots * g^{-1}}_{r \text{ times}}$. Hence, $(g^{-r})^{-s} = [(g^{-1})^r]^{-s}$. By the case where $n > 0$ and $m < 0$, we deduce that $(x^r)^{-s} = x^{-rs}$. Therefore: 
            \[(g^{-r})^{-s} = (g^{-1})^{-rs} = \underbrace{(g^{-1})^{-1} * \dots * (g^{-1})^{-1}}_{rs \text{ times}} = \underbrace{g * \dots * g}_{rs \text{ times}} = g^{rs}\]

            \item \underline{Case 5:} Suppose $m = 0$. Since $m = mn = 0$, it follows that: 
            \[(g^n)^m = \epsilon\]
            \[g^{nm} = \epsilon\]

            \item \underline{Case 6:} Suppose $n = 0$. By the same reasoning as case 5, we have that $n = mn = 0$. So: 
            \[(g^n)^m = \epsilon^m = \epsilon\]
            \[g^{mn} = \epsilon\]
        \end{itemize}

        Here, we notice that $\epsilon * \dots * \epsilon = \epsilon$ and $\epsilon^{-1} = \epsilon$, and so $\epsilon^m = \epsilon$. So, we showed that $(g^n)^m = g^{mn}$ for every $m, n \in \Z$. 
    \end{proof}
\end{mdframed}

\begin{note*}{}{}
    \begin{itemize}
        \item When we are working with an \underline{multiplicative group} $(G, \times)$, then $g^n$ means:
        \[g^n = \begin{cases}
            \underbrace{g \times \dots \times g}_{n \text{ times}} & n > 0 \\ 
            1 & n = 0 \\ 
            \underbrace{\frac{1}{g} \times \dots \times \frac{1}{g}}_{-n \text{ times}} & n < 0
        \end{cases}\]

        \item When we are working with an \underline{additive group} $(G, +)$, instead of writing $g^n$, we write $ng$. So, in $(G, +)$: 
        \[ng = \begin{cases}
            \underbrace{g + \dots + g}_{n \text{ times}} & n > 0 \\ 
            0 & n = 0 \\ 
            \underbrace{(-g) + \dots + (-g)}_{-n \text{ times}} & n < 0
        \end{cases}\]
        So, instead of writing $(g^{n})^m = g^{mn}$, we write $m(ng) = (mn)g$.

        \item For other valid groups, it depends on how you define the operation for the group. 
    \end{itemize}
\end{note*}

\begin{lemma}{}{}
    For every $m, n \in \Z$: 
    \[g^m * g^n = g^{m + n}\]
\end{lemma}

\begin{mdframed}
    \begin{proof}
        Like the previous proof, we will consider various cases depending on the signs of $m$ and $n$. Since it is easier to work with positive numbers, we will write $m = \text{sign}(m) r$ and $n = \text{sign}(n) s$ where $r = |m|$ and $s = |n|$, where: 
        \[\text{sign}: \R \to \{-1, 1\}\]
        \begin{itemize}
            \item \underline{Case 1:} Suppose $m$ and $n$ are positive. Then: 
            \[g^m * g^n = (\underbrace{g * \dots * g}_{m \text{ times}}) * (\underbrace{g * \dots * g}_{n \text{ times}}) = \underbrace{g * \dots * g}_{m + n \text{ times}} = g^{m + n}\]
    
            \item \underline{Case 2:} Suppose $m = -r$ ($m$ is negative), $n = s$ ($n$ is positive), $r < s$ ($m + n$ is positive). Then, by the previous case:
            \[g^r * g^{s - r} = g^s \implies g^{s - r} = (g^r)^{-1} * g^s = g^{-r} * g^s\]  
    
            \item \underline{Case 3:} Suppose $m = -r$, $n = s$, $r > s$ ($m + n$ is negative). Then, by the first case: 
            \begin{equation*}
                \begin{aligned}
                    g^s * g^{r - s} = g^r &\implies g^{r - s} = (g^s)^{-1} * g^r \\ 
                        &\implies (g^{r - s})^{-1} = ((g^s)^{-1} * g^r)^{-1} \\ 
                        &\implies g^{-(r - s)} = (g^r)^{-1} * ((g^s)^{-1})^{-1} \\ 
                        &\implies g^{-r + s} = g^{-r} * g^s
                \end{aligned}
            \end{equation*}
    
            \item \underline{Case 4:} Suppose $m = 0$. Then: 
            \[g^m * g^n  = \epsilon * g^n = g^n = g^{m + n}\]
    
            \item \underline{Case 5:} Suppose $n = 0$. Then: 
            \[g^m * g^n = g^m * \epsilon = g^m = g^{m + n}\]
        \end{itemize}
        By the above cases, we obtain the claim when $n \geq 0$ and $m \in \Z$. So: 
        \begin{itemize}
            \item \underline{Case 6:} Suppose $n = -s$ ($n$ is negative) and $s > 0$. Then: 
            \[g^{m - s} * g^s = g^m \implies g^{m - s} = g^m * (g^s)^{-1} \implies g^{m - s} = g^m * g^{-s}\]
        \end{itemize}
        This concludes the proof. 
    \end{proof}
\end{mdframed}


% --------------------------------------------------------- %
%                   NEW SECTION                             %
% --------------------------------------------------------- %
\newpage 
\section{More on Groups}
Now, we will look into the structure of groups. Specifically, we will consider how groups can be \emph{the same}. We'll also consider subgroups and generating groups and subgroups from elements and the order of elements. 

\subsection{Groups and Sizes}
A question we might consider is, for a given finite size, what different types of groups can we have?

\bigskip 

For instance, what can groups of size 2 look like? Well, we need to have a set of size 2. Denote this set $\{a, b\}$. We also need a binary operation for this set. We'll make use of a table to better demonstrate this.
\begin{center}
    \begin{tabular}{c|c c}
            & $a$ & $b$ \\ 
        \hline 
        $a$ &     &     \\ 
        $b$ &     &     
    \end{tabular}
\end{center}
Because we want a group, one of $a$ or $b$ needs to be an identity. Let's choose $a$ to be the identity. Then:
\begin{center}
    \begin{tabular}{c|c c}
            & $a$ & $b$ \\ 
        \hline 
        $a$ & $a$ & $b$ \\ 
        $b$ & $b$ & $a$ 
    \end{tabular}
\end{center}
At this point, $b$ has to be its own inverse ($a$ cannot be the inverse). Now, if $b$ is the identity element, then our table would look like\footnote{These aren't really meaningfully different since we just swapped $a$ and $b$.}:
\begin{center}
    \begin{tabular}{c|c c}
            & $a$ & $b$ \\ 
        \hline 
        $a$ & $b$ & $a$ \\ 
        $b$ & $a$ & $b$ 
    \end{tabular}
\end{center}

\subsection{Group Isomorphism}
Even if groups are not literally the same, they can be isomorphic (essentially the same as relabelling the elements of the set). Formally, the relabelling will be an invertible (or bijective) map between the sets of the groups that preserves the group structure. 

\bigskip 

Specifically, if we have a group $G_1$ with operation $*_1$ and a group $G_2$ with operation $*_2$, an isomorphism from $G_1$ to $G_2$ is a function $f: G_1 \to G_2$ such that $f$ is a bijection and:
\[f(g *_1 h) = f(g) *_2 f(h) \quad \forall g, h \in G_1\]
Formally: 
\begin{definition}{Isomorphism}{}
    Let $(S, *)$ and $(S', *')$ be binary algebraic structures. An \textbf{isomorphic of} $S$ with $S'$ is a one-to-one function $\Phi$ mapping $S$ onto $S'$ such that:
    \[\Phi(x * y) = \Phi(y) *' \Phi(y) \quad \forall x, y \in S\]
    If two groups have an isomorphism, then we say that they are isomorphic (essentially the same group). 
\end{definition}

\begin{corollary}{}{}
    It follows that:
    \[f^{-1}(k *_2 l) = f^{-1}(k) *_1 f^{-1}(l) \quad \forall k, l \in G_2\]
\end{corollary}

\begin{proof}
    We begin by noting 
\end{proof}

\subsubsection{Example: Isomorphism}
Consider the set $G = \{a, b\}$ with the corresponding table: 
\begin{center}
    \begin{tabular}{c|c c}
        $G$ & $a$ & $b$ \\ 
        \hline 
        $a$ & $a$ & $b$ \\ 
        $b$ & $b$ & $a$ 
    \end{tabular}
\end{center} 
This group $G$ is isomorphic to $\Z_2 = \{0, 1\}$ (where the operation is addition modulo 2). Our isomorphism is: 
\[f(a) = 0\]
\[f(b) = 1\]
The table corresponding to $\Z_2$ is:
\begin{center}
    \begin{tabular}{c|c c}
        $\Z_2$ & $0$ & $1$ \\ 
        \hline 
        $0$ & $0$ & $1$ \\ 
        $1$ & $1$ & $0$ 
    \end{tabular}
\end{center}
It follows that $f$ is isomorphic. 


\end{document}