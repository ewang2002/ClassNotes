\documentclass[letterpaper]{article}
\input{../../../preamble.tex}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage[hidelinks]{hyperref}

\pagestyle{fancy}
\fancyhf{}
\rhead{CSE 101}
\chead{Friday, January 28, 2022}
\lhead{Lecture 10}
\rfoot{\thepage}

\setlength{\parindent}{0pt}

\begin{document}

\section{Divide and Conquer}
The idea behind divide and conquer is as follows: 
\begin{enumerate}
    \item Break problems into similar pieces. 
    \item Solve pirces recursively. 
    \item Recombine the pieces to get an answer.
\end{enumerate}

\subsection{Integer Multiplication}
Suppose we're given two $n$-bit binary numbers and are asked to find their product. 

\subsubsection{Naive Algorithm}
The naive algorithm is to do multiplication like we would from elementary school. This runs in $\BigO(n^2)$ time because we need to write down $\BigO(n^2)$ bits of numbers to add (addition is done in linear time and is omitted).

\subsubsection{Improving the Algorithm: Two-Digit Multiplication}
Suppose we tried to multiply $ab$ by $cd$. This is normally done like so: 
\begin{verbatim}
                        a           b
                    x   c           d
                        -------------
                        ad         bd 
            +   ac      bc          0
                ---------------------
                ac      ad+bc      bd 
\end{verbatim}
This requires 4 one-digit multiplications and one addition. The \textbf{trick} is to compute $ac$, $bd$, $(a + b)(c + d)$. We note that (\emph{digit-wise}): 
\[bc + ad = (a + b)(c + d) - ac - bd\]
This requires \emph{3} one-digit multiplications and 4 addition/subtractions.

\subsubsection{Improving the Algorithm: Larger Base}
We can essentially generalize the above to larger bases with more digits. For example:
\begin{verbatim}
                a1      a2      a3   ...a(n/2)      a(n/2 + 1)   ...an
            x   b1      b2      b3   ...b(n/2)      b(n/2 + 1)   ...bn
                ------------------------------------------------------
                AC                      AD + BC                     BD 
\end{verbatim}
Where we can split the digits so that $A$ takes the first half of the first number, $B$ takes the second half of the first number, $C$ takes the first half of the second number, and $D$ takes the second half of the second number. 

\subsubsection{Formally}
Suppose we wanted to multiply $N$ by $M$. Then: 
\begin{enumerate}
    \item Let $X \approx \sqrt{N + M}$ be a power of 2. 
    \item Write $N = AX + B$ and $M = CX + D$. This can be done by just taking the high and low bits. 
    \item $NM = ACX^2 + (AD + BC)X + BD = ACX^2 + ((A + B)(C + D) - AC - BD)X + BD$, where the multiplication by $X$ are just bit shifts. 
\end{enumerate}

\subsubsection{Algorithm}
\begin{verbatim}
    ImprovedMult(N, M):
        Let X be 2^(log(N + M) / 2)
        Write N = AX + B, M = CX + D 
        P1 = Product(A, C)
        P2 = Product(B, D)
        P3 = Product(A + B, C + D)
        Return P1 X^2 + [P3 - P1 - P2]X + P3
\end{verbatim}
The first two lines take $\BigO(n)$ time, the three \code{Product} calls take $\BigO(n^2)$, and the last step takes $\BigO(n)$, for a total runtime of $\BigO(n^2)$. Despite there not being an asymptotic difference, note that this algorithm runs in $\BigO\left(\frac{3}{4}n^2 + n\right)$ time, whereas the naive algorithm runs in $\BigO(n^2)$ time, so there is a slight improvement. 

\subsubsection{Further Improvements}
Our algorithm still uses the naive algorithm (\code{Product}). However, we don't actually need to use this; we can just use our own implementation! We introduce \emph{Karatsuba's Algorithm}.

\begin{verbatim}
    KaratsubaMult(N, M)
        If N + M < 99
            Return Product(N, M)
        Let X be 2^(log(N + M) / 2)
        Write N = AX + B, M = CX + D 
        P1 = KaratsubaMult(A, C)
        P2 = KaratsubaMult(B, D)
        P3 = KaratsubaMult(A + B, C + D)
        Return P1 X^2 + [P3 - P1 - P2]X + P3
\end{verbatim}
Here, the pre-processing and post-processing still takes $\BigO(n)$ time. The three recursive calls are a bit tricky. 

\subsubsection{Runtime Recurrence}
Karatsuba's multiplication on inputs of size $n$ spends $\BigO(n)$ time, and then makes three recursive calls to problems of approximately half the size. If $T(n)$ is the runtime for $n$-bit inputs, we have the recurrence:
\[T(n) = \begin{cases}
    \BigO(1) & n = \BigO(1) \\ 
    3T(n / 2 + \BigO(1)) + \BigO(n) & \text{Otherwise}
\end{cases}\]
This isn't exactly a clean recurrence that we can solve. That being said, this runs in roughly $\BigO(n^{\log_{2}(3)})$ time. We will explain this later. 

\subsection{Generalization}
We will often get runtime recurrences with divide and conquer looking something like: 
\[T(n) = \begin{cases}
    \BigO(1) & n = \BigO(1) \\ 
    aT\left(\frac{n}{b} + \BigO(1)\right) + \BigO(n^d) & \text{Otherwise}
\end{cases}\]
Here, the second line is saying $a$ subproblems of size $\frac{n}{b}$. Note that the recursive subcalls are a constant \emph{fraction} of the size of the original. For example, if $T(n) = 2T(n - 1)$, then $T(n) = \BigO(2^n)$. 

\subsubsection{Tracking Recursive Calls}
We have: 
\begin{itemize}
    \item 1 recursive calls of size $n$
    \item $a$ recursive calls of size $n / b + O(1)$
    \item $a^2$ recursive calls of size $n / b^2 + O(1)$
    \item \dots
    \item $a^k$ recursive calls of size $n / b^k + O(1)$
\end{itemize}
So, the total runtime is: 
\begin{equation*}
    \begin{aligned}
        \text{Total Runtime } &= \sum_{k = 0}^{\log_{b}(n)} a^k \BigO\left(\left(\frac{n}{b^k}\right)^d\right) \\ 
            &= \BigO(n^d) \sum_{k = 0}^{\log_{b}(n)} \left(\frac{a}{b^d}\right)^k
    \end{aligned}
\end{equation*}

There are several cases to consider. 
\begin{enumerate}
    \item $a > b^d$: Increasing geometric series dominated by last term. The runtime is dominated by recursive calls at the bottom level. The runtime would be $O(n^{\log){b}(a)})$.
    \item $a < b^d$: Decreasing geometric series is dominated by the first term. Rutime is mostly based on the cleanup steps at the top level. The runtime is $O(n^d)$. 
    \item $a = b^d$: Every level of the recursion does the same amount of work. The runtime is $O(n^d \log(n))$. 
\end{enumerate}

\subsubsection{Master Theorem}
\begin{theorem}{Master Theorem}{}
    Let $T(n)$ be given by the recurrence:
    \[T(n) = \begin{cases}
        \BigO(1) & n = \BigO(1) \\ 
        aT\left(\frac{n}{b} + \BigO(1)\right) + \BigO(n^d) & \text{Otherwise}
    \end{cases}\]
    Then we have: 
    \[T(n) = \begin{cases}
        \BigO(n^{\log_{b}(a)}) & a > b^d \\ 
        \BigO(n^d \log(n)) & a = b^d \\ 
        \BigO(n^d) & a < b^d
    \end{cases}\]
\end{theorem}

\subsubsection{Example: Runtime}
Suppose that a divide and conquer algorithm needs to solve 4 recursive subproblems of half the size and do $\BigO(n^2)$ addition work. What is the runtime? 
\begin{mdframed}[]
    Here, we have 4 subproblems ($a = 4$) of half the size ($b = 2$) and we need to do $\BigO(n^2)$ additional work ($d = 2$). So, our recurrence is given by 
    \[T(n) = \begin{cases}
        \BigO(1) & n = \BigO(1) \\ 
        4T\left(\frac{n}{2} + \BigO(1)\right) + \BigO(n^2) & \text{Otherwise}
    \end{cases}\]
    Since $b^d = 2^2 = 4$ and $a = b^d$, it follows that the runtime is 
    \[\BigO(n^2 \log(n))\]
\end{mdframed}


\subsection{Matrix Multiplication}
Suppose we wanted to multiply $n \times n$ matrices.

\subsubsection{Recall}
If $AB = C$, then 
\[C_{i, j} = \sum_{k} A_{i, k} B_{k, j}\]
The naive algorithm computes this sum of $n$ terms for each of $n^2$ entries, so the runtime is given by $\BigO(n^3)$.

\subsubsection{Block Matrix Multiplication}
If we divide the matrix into blocks, we can get the product of the full matrix in terms of the products of the blocks. 
\[
    \begin{bmatrix}
        A & B \\ C & D
    \end{bmatrix} \begin{bmatrix}
        E & F \\ G & H
    \end{bmatrix} = \begin{bmatrix}
        AE + BG & AF + BH \\ 
        CE + DG & CF + DH 
    \end{bmatrix}
\]
Here, $A, B, C, \dots$ are $(n / 2) \times (n / 2)$ matrices. 

\subsubsection{Divide and Conquer Algorithm}
We can compute 8 products of $(n / 2) \times (n / 2)$ matrices, and do some addition to get an answer. The runtime is 
\[T(n) = 8T(n / 2) + \BigO(n^2)\]
The Master Theorem states that this would give us $\BigO(n^3)$ runtime. And, this isn't hard to see why. If we unrolled what this algorithm was doing, we're essentially just doing the naive multiplication.

\end{document}