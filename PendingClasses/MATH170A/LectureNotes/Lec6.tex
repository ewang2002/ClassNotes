\documentclass[letterpaper]{article}
\input{../../../preamble.tex}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage[hidelinks]{hyperref}

\pagestyle{fancy}
\fancyhf{}
\rhead{Math 170A}
\chead{Monday, January 23, 2023}
\lhead{Lecture 6}
\rfoot{\thepage}

\setlength{\parindent}{0pt}

\newcommand{\0}{\mathbf{0}}
\newcommand{\y}{\mathbf{y}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\e}{\mathbf{e}}

\begin{document}

\section{Positive Definite Systems \& Cholesky Decomposition (1.4)}
In the previous section, we learned about Gaussian Elimination with Pivoting (i.e., $A\x = \b \iff PA = LU$), which can be used to solve \emph{any} system with invertible matrix $A$. However, for some matrix $A$ with special properties, we can use a \emph{better} way to solve it, mainly for computational efficiency.  

\bigskip 

One such class of matrices are \textbf{positive definite} ones. In particular, if $A$ is $n \times n$, real, and satisfies the conditions 
\begin{enumerate}
    \item $A = A^T$ (i.e., symmetric), and 
    \item $\x^T A \x = \begin{bmatrix}
        x_1 & x_2 & \hdots & x_n
    \end{bmatrix} \begin{bmatrix}
        a_{11} & a_{12} & \hdots & a_{1n} \\ 
        a_{21} & a_{22} & \hdots & a_{2n} \\ 
        \vdots & \vdots & \ddots & \vdots \\ 
        a_{n1} & a_{n2} & \hdots & a_{nn}
    \end{bmatrix} \begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
    \end{bmatrix} = \cyclic{x^T, Ax} > 0$ for all nonzero $\x \in \R^n$, 
\end{enumerate}
then $A$ is said to be \textbf{positive definite}. 

\subsection{Properties of Positive Definite Matrix}
There are some useful properties of positive definite matrices. 

\begin{lemma}{}{}
    A positive definite matrix is invertible. 
\end{lemma}

\begin{theorem}{}{}
    Let $M$ be an invertible $n \times n$ matrix. Then, $A = M^T M$ is positive definite.  
\end{theorem}

\begin{theorem}{Cholesky Decomposition Theorem}{}
    Let $A$ be positive definite. Then, there exists an upper-triangular matrix $R$ such that \[A = R^T R.\] This is known as the Cholesky decomposition, and $R$ is called the \textbf{Cholesky factor} of $A$. In addition, this decomposition is unique if we choose $r_{ii} > 0$ for $i = 1, 2, \hdots, n$. 
\end{theorem}

\begin{mdframed}
    (Example: Cholesky Algorithm.) Suppose \[A = \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\ 
        a_{21} & a_{22} & a_{23} \\ 
        a_{31} & a_{32} & a_{33}
    \end{bmatrix}, \quad R = \begin{bmatrix}
        r_{11} & r_{12} & r_{13} \\ 
        0 & r_{22} & r_{23} \\ 
        0 & 0 & r_{33}
    \end{bmatrix},\]
    so that \[R^T = \begin{bmatrix}
        r_{11} & 0 & 0 \\ 
        r_{21} & r_{22} & 0 \\ 
        r_{31} & r_{32} & r_{33}
    \end{bmatrix}.\] Then, 
    \begin{equation*}
        \begin{aligned}
            \begin{bmatrix}
                a_{11} & a_{12} & a_{13} \\ 
                a_{21} & a_{22} & a_{23} \\ 
                a_{31} & a_{32} & a_{33}
            \end{bmatrix} &= \begin{bmatrix}
                r_{11} & 0 & 0 \\ 
                r_{21} & r_{22} & 0 \\ 
                r_{31} & r_{32} & r_{33}
            \end{bmatrix} \begin{bmatrix}
                r_{11} & r_{12} & r_{13} \\ 
                0 & r_{22} & r_{23} \\ 
                0 & 0 & r_{33}
            \end{bmatrix} \\ 
                &= \begin{bmatrix}
                    r_{11}^2 & r_{11}r_{12} & r_{11}r_{13} \\ 
                    r_{11}r_{12} & r_{12}^2 + r_{22}^2 & r_{12}r_{13} + r_{22} r_{23} \\ 
                    r_{11}r_{13} & r_{13}r_{12} + r_{23}r_{22} & r_{13}^2 + r_{23}^2 + r_{33}^2
                \end{bmatrix}
        \end{aligned}
    \end{equation*}
    One thing to notice is that this matrix is symmetric. So, when finding the values of each element, we only need to find the values of specific elements (one side). We can find the value of each element in $R$. Starting with the first row, we have 
    \begin{itemize}
        \item $r_{11}^2 = a_{11} \implies \boxed{r_{11} = \sqrt{a_{11}}}$.
        \item $r_{11}r_{12} = a_{12} \implies \boxed{r_{12} = \frac{a_{12}}{r_{11}}}$. 
        \item $r_{11}r_{13} = a_{13} \implies \boxed{r_{13} = \frac{a_{13}}{r_{11}}}$.
    \end{itemize}
    For the second row, we have 
    \begin{itemize}
        \item $r_{12}^2 + r_{22}^2 = a_{22} \implies \boxed{r_{22} = \sqrt{a_{22} - r_{12}^2}}$. 
        \item $r_{12}r_{13} + r_{22}r_{23} = a_{23} \implies \boxed{r_{23} = \frac{a_{23} - r_{12}r_{13}}{r_{22}}}$. 
    \end{itemize}
    For the third row, we have 
    \begin{itemize}
        \item $r_{13}^2 + r_{23}^2 + r_{33}^2 = a_{33} \implies \boxed{r_{33} = \sqrt{a_{33} - r_{13}^2 - r_{23}^2}}$. 
    \end{itemize}
\end{mdframed}

The above example is the Cholesky Algorithm for a $3 \times 3$ matrix. For the general case, suppose we have 
\[\underbrace{\begin{bmatrix}
    a_{11} & a_{12} & a_{13} & \hdots & a_{1n} \\ 
    a_{21} & a_{22} & a_{23} & \hdots & a_{2n} \\ 
    a_{31} & a_{32} & a_{33} & \hdots & a_{3n} \\ 
    \vdots & \vdots & \vdots & \ddots & \vdots \\ 
    a_{n1} & a_{n2} & a_{n3} & \hdots & a_{nn}
\end{bmatrix}}_{A} = \underbrace{\begin{bmatrix}
    r_{11} & 0 & 0 & \hdots & 0 \\ 
    r_{21} & r_{12} & 0 & \hdots & 0 \\ 
    r_{31} & r_{23} & r_{33} & \hdots & 0 \\ 
    \vdots & \vdots & \vdots & \ddots & \vdots \\ 
    r_{1n} & r_{2n} & r_{3n} & \hdots & r_{nn}
\end{bmatrix}}_{R^T} \underbrace{\begin{bmatrix} 
    r_{11} & r_{12} & r_{13} & \hdots & r_{1n} \\ 
    0 & r_{22} & r_{23} & \hdots & r_{2n} \\ 
    0 & 0 & r_{33} & \hdots & r_{3n} \\ 
    \vdots & \vdots & \vdots & \ddots & \vdots \\ 
    0 & 0 & 0 & \hdots & r_{nn}
\end{bmatrix}}_{R}.\]
Here, 
\[a_{ij} = \begin{cases}
    \sum_{k = 1}^{i} r_{ki} r_{kj} & i < j \\ 
    \sum_{k = 1}^{i} r_{ki}^2 & i = j \\ 
    \sum_{k = 1}^{j} r_{ki} r_{kj} & i > j
\end{cases}.\]
From this, we're able to solve $r_{ii}$: 
\[r_{ii} = \sqrt{a_{ii} - \sum_{k = 1}^{i - 1} r_{ki}^2}.\]
After solving for $r_{ii}$, we can find $r_{ij}$:
\[r_{ij} = \frac{\left(a_{ij} - \sum_{k = 1}^{i - 1} r_{ki} r_{kj}\right)}{r_{ii}}\]
for $j = i + 1, \hdots, n$, noting that for $j < i$, $r_{ij} = 0$. 

\bigskip 

In general, Cholesky is doing half of the LU decomposition work: $\BigO(n^3)$. 

\end{document}