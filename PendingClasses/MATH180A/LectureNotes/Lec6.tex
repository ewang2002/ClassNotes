\documentclass[letterpaper]{article}
\input{../../../preamble.tex}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage[hidelinks]{hyperref}

\pagestyle{fancy}
\fancyhf{}
\rhead{MATH 180A}
\chead{Monday, April 11, 2022}
\lhead{Lecture 6}
\rfoot{\thepage}

\setlength{\parindent}{0pt}

\begin{document}
\section{Combinatorics}
\subsection{Combinations}
Suppose we have a set $A$ of size $|A| = n$. How many subsets of $A$ are there? How many of these are of size $k$? 

\begin{definition}{}{}
    The number of ways to choose $k$ elements from a set of $n$ distinguishable objects is denoted by $\binom{n}{k}$.
\end{definition}
\textbf{Remarks:}
\begin{itemize}
    \item If $|A| = n$, then there are $\binom{n}{k}$ subsets of $S \subset A$ of size $|S| = k$, since each subset corresponds to a way of choosing $k$ elements from the set of $n$ elements. 
    \item The number $\binom{n}{k}$ is also known as the \textbf{binomial coefficient}.
\end{itemize}

\begin{theorem}{}{}
    For $0 \leq k \leq n$, we have 
    \[\binom{n}{k} = \frac{n!}{k!(n - k)!}.\]
\end{theorem}
\textbf{Remarks:}
\begin{itemize}
    \item Recall that $0! = 1$.
    \item Recall that $\frac{n!}{(n - k)!} = (n)_k$. 
    \item So, we can say that $\binom{n}{k} = \frac{(n)_k}{k!}$.
\end{itemize}
One thing to note is that the relationship 
\[\binom{n}{k} = \binom{n - 1}{k - 1} + \binom{n - 1}{k}\]
gives us a recursive method of computing binomial coefficients. This is known as the famous \textbf{Pascal's Triangle}.
One thing to notice is that 
\[\frac{n!}{k!(n - k)!} = \frac{n!}{(n - k)!(n - (n - k))!}\]
and so it follows that 
\[\binom{n}{k} = \binom{n}{n - k}.\]

\begin{mdframed}[]
    (Example.) Why does a Four of a Kind beat a Full House in Poker? 
    \begin{itemize}
        \item Recall that there are 52 cards in a deck. In Poker, you get 5 cards. 
        \item A Four of a Kind means that you get 4 of one of the kind of cards and one other card (e.g. all 4 aces and a 2).
        \item A Full House is when you get 3 of one kind and 2 of the other (e.g. 3 aces and 2 3's.).
    \end{itemize}

    \begin{mdframed}[]
        How many ways can we get a Full House? 
        \begin{itemize}
            \item We need to get two types of cards. There are $13 \cdot 12$ ways to do this.
            \item Now, we need to make the full house. For one of the cards, there are 4 different suits, of which we want 3 suits. Thus, $\binom{4}{3}$.
            \item For the other card, there are again 4 different suits, of which we want 2 suits. Thus, $\binom{4}{2}$. 
        \end{itemize}
        This gives us 
        \[13 \cdot 12 \cdot \binom{4}{3} \binom{4}{2}.\]
        How many ways can we get a Four of a Kind? 
        \begin{itemize}
            \item There are 13 ways to choose the first card (that we need 4 of one kind for).
            \item There are now 48 cards left. We just need to pick one card to be our extra card.
        \end{itemize}
        This gives us 
        \[13 \cdot 48.\]
        The probability of a Full House is given by 
        \[\frac{13 \cdot 12 \cdot \binom{4}{3} \binom{4}{2}}{\binom{52}{5}} \approx 0.14\%.\]
        The probability of a Four of a Kind is given by 
        \[\frac{13 \cdot 48}{\binom{52}{5}} \approx 0.024\%.\]
        For both of these cases, the $\binom{52}{5}$ came from the fact that we get 5 cards from a deck of 52. 
    \end{mdframed}
\end{mdframed}

\subsection{Binomial Distribution}
There is an important probability distribution related to the binomial coefficients, called the \textbf{Binomial Distribution}. But, we first need to describe the concept of a \textbf{Bernoulli trial}.
\begin{definition}{}{}
    A \textbf{Bernoulli trial} is a simple experiment that is either a \emph{success} or \emph{failure}. More specifically, it is a discrete random variable that either takes the value 1 (success) or 0 (failure).
\end{definition}
Moreover, a $\text{Bernoulli}(p)$ trial is one in which the probability of success is $p$. Hence, its PDF is 
\[\PR(X = 1) = p\]
and 
\[\PR(X = 0) = 1 - p.\]
For example, flipping ``Tails'' when tossing a fair coin is a $\text{Bernoulli}(1 / 2)$ trial. 

\bigskip 

We haven't defined what independence means in probability, but informally, a series of events $E_1, \dots, E_n$ are independent if their outcomes ``do not affect'' each other. For example, when we flip a coin, whatever we get on the first flip won't affect what we get on the second flip. So, in this case, we have 
\[\PR\left(\bigcap_{i = 1}^{n} E_i\right) = \prod_{i = 1}^{n} \PR(E_i).\]
In other words, the probability that they all occur is just the product of the individual probabilities. 

\begin{definition}{}{}
    Let $n \geq 1$ be an integer and $P \in [0, 1]$. Let $N$ be the number of ``successes'' in a series of $n$ independent Bernoulli($p$) trials. Then, we say that $N$ has the Binomial($n, p$) distribution.
\end{definition}
Its PMF\footnote{Probability Mass Function} is given by 
\[\PR(N = k) = \binom{n}{k}p^k (1 - p)^{n - k}\]
for $0 \leq k \leq n$ and $\PR(N = x) = 0$ otherwise. To see why this is the case,
\begin{itemize}
    \item There are $\binom{n}{k}$ ways to choose which $k$ of the $n$ trials will be successful. Each of these $k$ trials will be a success with probability $p$, and each of the remaining $n - k$ trials will be a failure with probability $1 - p$. 
    \item Since these trials are independent, we can multiply everything together: each of the $\binom{n}{k}$ outcomes that would cause $N - k$ to have probability $p^k (1 - p)^{n - k}$. 
\end{itemize}
How do we see that this is a legitimate probability distribution? In other words, how do we know that this all adds up to 1? 
\begin{theorem}{Binomial Theorem}{}
    \[(a + b)^n = \sum_{k = 0}^{n} \binom{n}{k} a^k b^{n - k}.\]
\end{theorem}
With this in mind, we have 
\[\sum_{k = 0}^{n} \PR(N = k) = \sum_{k = 0}^{n} \binom{n}{k} p^k (1 - p)^{n - k} = (p + (1 - p))^n = 1^n = 1.\]

\end{document}