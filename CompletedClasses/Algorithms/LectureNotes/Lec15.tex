\documentclass[letterpaper]{article}
\input{../../../preamble.tex}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage[hidelinks]{hyperref}

\pagestyle{fancy}
\fancyhf{}
\rhead{CSE 101}
\chead{Wednesday, February 09, 2022}
\lhead{Lecture 15}
\rfoot{\thepage}

\setlength{\parindent}{0pt}

\begin{document}

\section{Greedy Algorithm}
We continue our discussion of greedy algorithms.

\subsection{Problem: Huffman Codes}
Suppose, for simplicity, that our alphabet was $a, b, c, d$. If we have a string of letters that we wanted to encode, say, abcdacbdadccb, we would need to store it in the computer as a bunch of \code{0}'s and \code{1}'s. So, we can let $a = \code{00}$, $b = \code{01}$, $c = \code{10}$, and $d = \code{11}$. Then, we can encode the string above like so 
\begin{center}
    \begin{tabular}{c c c c c c c c c c c c c}
        a & b & c & d & a & c & b & d & a & d & c & c & b \\ 
        \hline 
        00 & 01 & 10 & 11 & 00 & 10 & 01 & 11 & 00 & 11 & 10 & 10 & 01 
    \end{tabular}
\end{center}
With this encoding, it's easy to decode as long as we have the mapping. This has an issue, though: suppose we have the string aaaaaaaaaaaabaacaa. Then, it's obvious that we're encoding \code{a} many more times than the other three letters. So, is there a way to map \code{a} to one bit instead of two bits so we can code the \code{a}'s with fewer bits? 

\bigskip 

Suppose $a = \code{0}$, $b = \code{1}$, $c = \code{00}$, and $d = \code{01}$. Then, we have soem ambiguity. If we had the string \code{00}, then this would map to either \code{aa} or \code{c}. So, how do we make this work?  

\bigskip 

Now suppose $a = \code{0}$, $c = \code{10}$, $b = \code{110}$, and $d = \code{111}$. Then, if we have the string \code{00001000101100001110100}, how do we decode this? Well, if we start reading from the left, then we note that: 
\begin{itemize}
    \item We first read the \code{0}. This can only map to \code{a} as the other characters start with \code{1}.
    \item Doing this three more times, we end up with \code{aaa}. Now, we consider the character \code{1}. There are three mappings where \code{1} starts first: \code{b}, \code{c}, and \code{d}. So, we read in the next character, \code{0}. Here, we note that this can only be \code{c} as \code{c} maps to a string \code{10} but the other two mappings start with \code{11}. 
    \item We can repeat this process until we're done reading the string.
\end{itemize}
At the end, the decoded string is: 
\begin{center}
    \begin{tabular}{c c c c c c c c c c c c c c c c}
        0 & 0 & 0 & 0 & 10 & 0 & 0 & 10 & 110 & 0 & 0 & 0 & 111 & 0 & 10 & 0 \\ 
        \hline 
        a & a & a & a & c  & a & a & c  & b   & a & a & a & d   & a & c  & a
    \end{tabular}
\end{center}
This is an example of \textbf{prefix-free encoding}: that is, no letters $x$, $y$ is the encoding of $x$ a prefix of the encoding of $y$. 

\bigskip 

Our problem statement is as follows: Given an alphabet where each letter $x$ has a frequency\footnote{The number of times it appears in the string.} $f(x)$, we want a prefix-free encoding of the alphabet such that the encoding length $\sum_{x} f(x) \cdot |\text{encoding}(x)|$ is minimized.


\subsubsection{Rewording the Problem}
Our first observation is that there is a binary tree representation of the prefix-free encoding, where taking the left branch is the \code{0} bit and the right branch is the \code{1} bit. Then, you can place letters in locations corresponding to their encoding. For example, the tree representation of the above prefix-free encoding is: 
\begin{verbatim}
                        /\
                       /  \ 
                      a   /\ 
                         /  \ 
                        c   /\ 
                           /  \ 
                          b    d 
\end{verbatim} 
So, for a prefix-free encoding, the letters are at the leaves of the binary tree. So, we can rephrase the problem
\[\sum_x f(x) \cdot \text{depth}(x)\]
where the depth is the same as the length of the encoding of some $x$. 

\subsubsection{Easy Case}
Suppose we fix the tree structure. That is, our tree must look like 
\begin{verbatim}
    /\
   /  \ 
  ?   /\ 
     /  \ 
    ?   /\ 
       /  \ 
      ?    ?
\end{verbatim} 
We know that we have four letters where one letter has depth 1, one letter has depth 2, and two letters have depth 3. Now, the best way to assign letters is to assign the \emph{high frequency} letters to the smaller depths. To see this, we note that 
\begin{verbatim}
    \ 
     \ 
     /\ 
    /  \
   x    . 
         . 
          . 
           \ 
            y 
\end{verbatim}
To compare $x$ and $y$, we want to compare
\[d_1 f(x) + d_2 f(y)\]
with 
\[d_2 f(x) + d_1 f(y)\]
where $d_i$ is the depth. Then, we can factor this out like so 
\[(d_1 - d_2)(f(x) - f(y))\]
If $x$ has lower frequency and it was stored at the smaller depth, then switching them would decrease the total expense. So, if we have a fixed tree, there is a greedy algorithm:
\begin{itemize}
    \item Sort letters by frequency. 
    \item Sort location by depth. 
    \item Assign letters with higher frequencies to lower depth.
\end{itemize}

\subsubsection{Observation}
Two of the deepest elements are siblings. Note that the deepest elements correspond to the letters with little frequency. Thus, the key insight here is 
\begin{mdframed}[]
    The two least frequent letters in the alphabet might as well be siblings. 
\end{mdframed}

\subsubsection{An Example}
Suppose we had: 
\begin{itemize}
    \item 30 copies of \code{a}
    \item 15 copies of \code{b}
    \item 25 copies of \code{c}
    \item 50 copies of \code{d}
    \item 65 copies of \code{e}
\end{itemize}
We can assume that \code{b} and \code{c} are siblings since they have the lowest frequencies. So: 
\begin{verbatim}
      /\ 
     /  \ 
    b    c
\end{verbatim}
Next, note that we can call the parent of \code{b} and \code{c} \emph{\code{b} or \code{c}}, and thus treat it as a ``different letter.'' We can also give this ``different letter'' a value of \code{40}, as \code{b} and \code{c} appear 40 times. Thus, we now have the alphabet:
\begin{itemize}
    \item 30 copies of \code{a}
    \item 40 copies of \code{b} or \code{c}
    \item 50 copies of \code{d}
    \item 65 copies of \code{e}
\end{itemize}
Notice that we can repeat the process again. In particular, notice that \code{b} or \code{c} \emph{and} \code{a} are siblings since they have the lowest frequencies. So: 
\begin{verbatim}
      /\ 
     /  \ 
    a   /\ 
       /  \ 
      b    c
\end{verbatim}
Again, we can call the parent of \code{a} and \code{b} or \code{c} \emph{\code{a} or \code{b} or \code{c}}, and treat it as a ``different letter'' with the value being 70. Thus, we now have the alphabet:
\begin{itemize}
    \item 70 copies of \code{a} or \code{b} or \code{c}
    \item 50 copies of \code{d}
    \item 65 copies of \code{e}
\end{itemize}
We now repeat this process. Here, we see that \code{d} and \code{e} are siblings since they have the lowest frequencies. So: 
\begin{verbatim}
    /\     /\ 
   /  \   /  \ 
  d    e a   /\ 
            /  \ 
           b    c
\end{verbatim}
We call the parent of \code{d} and \code{e} \emph{\code{d} or \code{e}} and give it a value of 115. Thus, we now have the alphabet:
\begin{itemize}
    \item 70 copies of \code{a} or \code{b} or \code{c}
    \item 115 copies of \code{d} or \code{e}
\end{itemize}
But since there are only two ``letters,'' we can just pair them up: 
\begin{verbatim}
         /\ 
        /  \
       /    \ 
      /      \
     /\      /\ 
    /  \    /  \ 
   d    e  a   /\ 
              /  \ 
             b    c
\end{verbatim}

\subsubsection{Algorithm}
The algorithm is as follows, where 
\begin{itemize}
    \item $S$ is the alphabet. 
    \item $f$ is the frequency function. 
\end{itemize}

\begin{verbatim}
    HuffmanCode(S):
        while |S| > 1:
            Let x, y be least frequent elements of S 
            Create z with f(z) = f(x) + f(y)
            Make x, y children of z 
            Remove x, y from S 
            Add z 

        Return element in S 
\end{verbatim}

If $S$ has size $n$, then the runtime is $\BigO(n^2)$. This is because we need to iterate over the \code{while} loop, and for each iteration we need to do a linear scan. But, we can make use of a \emph{priority queue} to optimize our algorithm.

\begin{verbatim}
    OptimizedHuffmanCode(S):
        Insert S into priority queue Q 
        while |Q| > 1:
            Let x = deleteMin(Q)
            Let y = deleteMin(Q)
            Create z with f(z) = f(x) + f(y)
            Make x, y children of z 
            Q.Insert(z)

        Return deleteMin(Q) 
\end{verbatim}
This runs in $\BigO(n\log(n))$. 

\subsubsection{Takeaways}
There are some takeaways from this. 
\begin{itemize}
    \item To find a greedy decision procedure, a good thing to do first is to find a safe first step. The safe first step in this algorithm is that the two elements might as well be siblings. Once you made that decision, you need to reduce the problem back to a copy of the original problem. 
    \item Often, in order to turn something into a greedy algorithm, you might need to rearrange the way the problem is phrased; i.e. rephrase the problem. The original phrasing of this problem -- finding the prefix-free encodings of letters of this alphabet which minimize the total encoding length -- is awkward to phrase as a greedy algorithm. The idea that we can pair two elements as siblings would be incredibly awkward to phrase under the original problem statement.  
\end{itemize}



\end{document}